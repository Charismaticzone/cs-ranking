{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate MNIST Largest Number Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "from keras import backend as K\n",
    "\n",
    "# Set random seeds EVERYWHERE:\n",
    "seed = 123\n",
    "rand = np.random.RandomState(seed)\n",
    "os.environ['PYTHONHASHSEED'] = '{}'.format(rand.randint(2**32))\n",
    "np.random.seed(rand.randint(2**32))\n",
    "rn.seed(rand.randint(2**32))\n",
    "tf.set_random_seed(rand.randint(2**32))\n",
    "\n",
    "# Make TensorFlow deterministic:\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "# Even after all this machinery the weights are only Îµ-close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How many MNIST numbers should be available for sampling in the end?\n",
    "test_size = 60000\n",
    "\n",
    "# Final dataset:\n",
    "n_train = 10000\n",
    "n_test = 10000\n",
    "n_objects = 10\n",
    "n_features = 128\n",
    "\n",
    "output_name = 'largest_mnist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original', data_home=\"/home/kiudee/Daten/\")\n",
    "X_raw = mnist['data'].reshape(-1, 28, 28) / 255.\n",
    "y = mnist['target']\n",
    "num_classes = len(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f14181accf8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRvnYImYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f140c1feda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(X_raw[126])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Split into Train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_raw_train, X_raw_test, y_train, y_test = train_test_split(\n",
    "    X_raw, y,\n",
    "    test_size=test_size,\n",
    "    random_state=rand,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Train CNN for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import Model\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Dropout, Flatten, BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "\n",
    "regularizer = l2(1e-4)\n",
    "\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "#model = Sequential()\n",
    "inp = Input(input_shape)\n",
    "tr = Conv2D(32, kernel_size=(5, 5),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape,\n",
    "                 kernel_regularizer=regularizer,\n",
    "                 use_bias=False)(inp)\n",
    "tr = MaxPooling2D()(tr)\n",
    "tr = BatchNormalization()(tr)\n",
    "tr = Conv2D(64, (5, 5), activation='relu',\n",
    "                 kernel_regularizer=regularizer,\n",
    "                 use_bias=False)(tr)\n",
    "tr = MaxPooling2D()(tr)\n",
    "tr = BatchNormalization()(tr)\n",
    "tr = Flatten()(tr)\n",
    "tr = Dense(1024, activation='relu', kernel_regularizer=regularizer, use_bias=False)(tr)\n",
    "tr = BatchNormalization()(tr)\n",
    "tr = Dense(n_features, activation='relu', kernel_regularizer=regularizer)(tr)\n",
    "tr = BatchNormalization()(tr)\n",
    "out = Dense(num_classes, activation='softmax', kernel_regularizer=regularizer)(tr)\n",
    "model = Model(inputs=inp, outputs=out)\n",
    "model_feat = Model(inputs=inp, outputs=tr)\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def schedule(ep):\n",
    "    if ep < 10:\n",
    "        return 1e-2\n",
    "    if ep < 20:\n",
    "        return 1e-3\n",
    "    return 1e-4\n",
    "lrschedule = LearningRateScheduler(schedule)\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=SGD(lr=1e-3, momentum=0.9, nesterov=True), metrics=['accuracy'],\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      "9000/9000 [==============================] - 2s 254us/step - loss: 0.3625 - acc: 0.9294 - val_loss: 0.2387 - val_acc: 0.9620\n",
      "Epoch 2/30\n",
      "9000/9000 [==============================] - 1s 166us/step - loss: 0.1809 - acc: 0.9873 - val_loss: 0.2162 - val_acc: 0.9740\n",
      "Epoch 3/30\n",
      "9000/9000 [==============================] - 2s 174us/step - loss: 0.1501 - acc: 0.9968 - val_loss: 0.2011 - val_acc: 0.9790\n",
      "Epoch 4/30\n",
      "9000/9000 [==============================] - 2s 168us/step - loss: 0.1423 - acc: 0.9984 - val_loss: 0.2052 - val_acc: 0.9780\n",
      "Epoch 5/30\n",
      "9000/9000 [==============================] - 2s 174us/step - loss: 0.1380 - acc: 0.9990 - val_loss: 0.2018 - val_acc: 0.9770\n",
      "Epoch 6/30\n",
      "9000/9000 [==============================] - 1s 164us/step - loss: 0.1358 - acc: 0.9993 - val_loss: 0.1960 - val_acc: 0.9750\n",
      "Epoch 7/30\n",
      "9000/9000 [==============================] - 2s 171us/step - loss: 0.1329 - acc: 1.0000 - val_loss: 0.1998 - val_acc: 0.9790\n",
      "Epoch 8/30\n",
      "9000/9000 [==============================] - 2s 170us/step - loss: 0.1316 - acc: 1.0000 - val_loss: 0.1966 - val_acc: 0.9800\n",
      "Epoch 9/30\n",
      "9000/9000 [==============================] - 2s 170us/step - loss: 0.1303 - acc: 1.0000 - val_loss: 0.1951 - val_acc: 0.9780\n",
      "Epoch 10/30\n",
      "9000/9000 [==============================] - 1s 166us/step - loss: 0.1294 - acc: 1.0000 - val_loss: 0.1950 - val_acc: 0.9760\n",
      "Epoch 11/30\n",
      "9000/9000 [==============================] - 2s 174us/step - loss: 0.1287 - acc: 1.0000 - val_loss: 0.1938 - val_acc: 0.9780\n",
      "Epoch 12/30\n",
      "9000/9000 [==============================] - 2s 171us/step - loss: 0.1286 - acc: 1.0000 - val_loss: 0.1935 - val_acc: 0.9790\n",
      "Epoch 13/30\n",
      "9000/9000 [==============================] - 2s 170us/step - loss: 0.1286 - acc: 1.0000 - val_loss: 0.1936 - val_acc: 0.9810\n",
      "Epoch 14/30\n",
      "9000/9000 [==============================] - 2s 167us/step - loss: 0.1284 - acc: 1.0000 - val_loss: 0.1933 - val_acc: 0.9810\n",
      "Epoch 15/30\n",
      "9000/9000 [==============================] - 2s 176us/step - loss: 0.1283 - acc: 1.0000 - val_loss: 0.1933 - val_acc: 0.9810\n",
      "Epoch 16/30\n",
      "9000/9000 [==============================] - 2s 169us/step - loss: 0.1283 - acc: 1.0000 - val_loss: 0.1931 - val_acc: 0.9820\n",
      "Epoch 17/30\n",
      "9000/9000 [==============================] - 2s 169us/step - loss: 0.1282 - acc: 1.0000 - val_loss: 0.1929 - val_acc: 0.9800\n",
      "Epoch 18/30\n",
      "9000/9000 [==============================] - 1s 166us/step - loss: 0.1280 - acc: 1.0000 - val_loss: 0.1930 - val_acc: 0.9790\n",
      "Epoch 19/30\n",
      "9000/9000 [==============================] - 1s 166us/step - loss: 0.1279 - acc: 1.0000 - val_loss: 0.1929 - val_acc: 0.9800\n",
      "Epoch 20/30\n",
      "9000/9000 [==============================] - 1s 166us/step - loss: 0.1278 - acc: 1.0000 - val_loss: 0.1929 - val_acc: 0.9780\n",
      "Epoch 21/30\n",
      "9000/9000 [==============================] - 2s 180us/step - loss: 0.1277 - acc: 1.0000 - val_loss: 0.1929 - val_acc: 0.9780\n",
      "Epoch 22/30\n",
      "9000/9000 [==============================] - 2s 171us/step - loss: 0.1277 - acc: 1.0000 - val_loss: 0.1928 - val_acc: 0.9800\n",
      "Epoch 23/30\n",
      "9000/9000 [==============================] - 2s 169us/step - loss: 0.1277 - acc: 1.0000 - val_loss: 0.1929 - val_acc: 0.9790\n",
      "Epoch 24/30\n",
      "9000/9000 [==============================] - 2s 173us/step - loss: 0.1277 - acc: 1.0000 - val_loss: 0.1928 - val_acc: 0.9790\n",
      "Epoch 25/30\n",
      "9000/9000 [==============================] - 2s 180us/step - loss: 0.1277 - acc: 1.0000 - val_loss: 0.1928 - val_acc: 0.9800\n",
      "Epoch 26/30\n",
      "9000/9000 [==============================] - 2s 181us/step - loss: 0.1278 - acc: 1.0000 - val_loss: 0.1930 - val_acc: 0.9800\n",
      "Epoch 27/30\n",
      "9000/9000 [==============================] - 2s 170us/step - loss: 0.1279 - acc: 1.0000 - val_loss: 0.1930 - val_acc: 0.9800\n",
      "Epoch 28/30\n",
      "9000/9000 [==============================] - 2s 172us/step - loss: 0.1277 - acc: 1.0000 - val_loss: 0.1930 - val_acc: 0.9790\n",
      "Epoch 29/30\n",
      "9000/9000 [==============================] - 1s 164us/step - loss: 0.1277 - acc: 1.0000 - val_loss: 0.1929 - val_acc: 0.9800\n",
      "Epoch 30/30\n",
      "9000/9000 [==============================] - 2s 172us/step - loss: 0.1277 - acc: 1.0000 - val_loss: 0.1930 - val_acc: 0.9800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f11fd51ca20>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_raw_train[..., None], keras.utils.to_categorical(y_train, num_classes),\n",
    "          batch_size=64,\n",
    "          epochs=30,\n",
    "          verbose=1,\n",
    "          validation_split=0.1,\n",
    "          callbacks=[lrschedule]\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use CNN to transform images to high-level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_raw_feat = model_feat.predict(X_raw_test[...,None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_total = n_train + n_test\n",
    "largest_numbers = rand.randint(1, num_classes, size=n_total)\n",
    "X = np.empty((n_total, n_objects, n_features))\n",
    "y_number = np.empty((n_total, n_objects), dtype=int)\n",
    "for i in range(n_total):\n",
    "    remaining = X_raw_feat[y_test <= largest_numbers[i]]\n",
    "    while True:\n",
    "        indeces = rand.choice(len(remaining), size=n_objects, replace=False)\n",
    "        X[i] = remaining[indeces]\n",
    "        y_number[i] = y_test[y_test <= largest_numbers[i]][indeces]\n",
    "        if largest_numbers[i] in y_number[i]:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = (y_number == largest_numbers[:, None]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, random_state=rand,\n",
    "                                                    test_size=n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"{}_X_train.npy\".format(output_name), X_train)\n",
    "np.save(\"{}_X_test.npy\".format(output_name), X_test)\n",
    "np.save(\"{}_Y_train.npy\".format(output_name), Y_train)\n",
    "np.save(\"{}_Y_test.npy\".format(output_name), Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gpflow]",
   "language": "python",
   "name": "conda-env-gpflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
