{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import inspect\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from csrank.util import setup_logging\n",
    "from experiments.util import lp_metric_dict\n",
    "import numpy as np\n",
    "from experiments.dbconnection import DBConnector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PATH = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "log_path = os.path.join(DIR_PATH, 'logs', 'results.log')\n",
    "setup_logging(log_path=log_path)\n",
    "logger = logging.getLogger('ResultParsing')\n",
    "config_file_path = os.path.join(DIR_PATH, 'config', 'clusterdb.json')\n",
    "datasets = ['synthetic_dc', 'mnist_dc', 'tag_genome_dc', \"letor_dc\", \"sushi_dc\"]\n",
    "DATASET = datasets[4]\n",
    "learning_problem = \"discrete_choice\"\n",
    "results_table = 'results.{}'.format(learning_problem)\n",
    "schema = 'masterthesis'\n",
    "start=3\n",
    "select_jobs = \"SELECT learner_params, dataset_params, hp_ranges, {0}.job_id, dataset, learner, {3} from {0} INNER JOIN {1} ON {0}.job_id = {1}.job_id where {1}.dataset=\\'{2}\\'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = DBConnector(config_file_path=config_file_path, is_gpu=False, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_result = \"UPDATE results.discrete_choice set cluster_id = %s, CategoricalAccuracy = %s, CategoricalTopK2 = %s, CategoricalTopK3 = %s, CategoricalTopK4 = %s, CategoricalTopK5 = %s, CategoricalTopK6 = %s  where job_id= %s\"\n",
    "values = (6636228, 0.4343, 0.6603, 0.8295, 0.9504, 1.0000, 1.0000,479)\n",
    "self.init_connection()\n",
    "self.cursor_db.execute(update_result, tuple(values))\n",
    "self.close_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_letor_string(dp):\n",
    "    y =  str(dp['year']) \n",
    "    n = str(dp['n_objects'])\n",
    "    return \"y_{}_n_{}\".format(y,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CategoricalAccuracy, CategoricalTopK2, CategoricalTopK3, CategoricalTopK4, CategoricalTopK5, CategoricalTopK6'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = list(lp_metric_dict[learning_problem].keys())\n",
    "keys[-1] = keys[-1].format(6)\n",
    "metrics = ', '.join([x for x in keys])\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT learner_params, dataset_params, hp_ranges, results.discrete_choice.job_id, dataset, learner, CategoricalAccuracy, CategoricalTopK2, CategoricalTopK3, CategoricalTopK4, CategoricalTopK5, CategoricalTopK6 from results.discrete_choice INNER JOIN masterthesis.avail_jobs ON results.discrete_choice.job_id = masterthesis.avail_jobs.job_id where masterthesis.avail_jobs.dataset='sushi_dc'\n"
     ]
    }
   ],
   "source": [
    "self.init_connection()\n",
    "avail_jobs = \"{}.avail_jobs\".format(self.schema)\n",
    "select_st = select_jobs.format(results_table, avail_jobs, DATASET, metrics)\n",
    "print(select_st)\n",
    "self.cursor_db.execute(select_st)\n",
    "data = []\n",
    "for job in self.cursor_db.fetchall():\n",
    "    job = dict(job)\n",
    "    n_hidden = job['hp_ranges'][job['learner']].get(\"n_hidden\", [])\n",
    "    if job['hp_ranges'][job['learner']].get(\"n_hidden_set_layers\", None)==[1,8]:\n",
    "        job['learner'] = job['learner']+'_shallow'\n",
    "    elif n_hidden==[1,4] or n_hidden==[1,5]:\n",
    "        job['learner'] = job['learner']+'_shallow'\n",
    "        \n",
    "    if job['learner_params'].get(\"add_zeroth_order_model\", False):\n",
    "        job['learner'] = job['learner']+'_zero'\n",
    "    if \"letor\" in job['dataset']:\n",
    "        job['dataset'] = get_letor_string(job['dataset_params'])\n",
    "    elif \"sushi\" in job['dataset']:\n",
    "        job['dataset'] =  job['dataset']\n",
    "    else:\n",
    "        job['dataset'] = job['dataset_params']['dataset_type']\n",
    "    job['learner'] = job['learner'].upper()\n",
    "    job['dataset'] = job['dataset'].upper()\n",
    "    values = list(job.values())\n",
    "    keys = list(job.keys())\n",
    "    columns = keys[start:]\n",
    "    vals = values[start:]\n",
    "    data.append(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT learner_params, dataset_params, hp_ranges, results.discrete_choice.job_id, dataset, learner, CategoricalAccuracy, CategoricalTopK2, CategoricalTopK3, CategoricalTopK4, CategoricalTopK5, CategoricalTopK6 from results.discrete_choice INNER JOIN pymc3.avail_jobs ON results.discrete_choice.job_id = pymc3.avail_jobs.job_id where pymc3.avail_jobs.dataset='sushi_dc'\n"
     ]
    }
   ],
   "source": [
    "self.init_connection()\n",
    "avail_jobs = \"{}.avail_jobs\".format(\"pymc3\")\n",
    "select_st = select_jobs.format(results_table, avail_jobs, DATASET, metrics)\n",
    "print(select_st)\n",
    "self.cursor_db.execute(select_st)\n",
    "for job in self.cursor_db.fetchall():\n",
    "    job = dict(job)\n",
    "    if \"letor\" in job['dataset']:\n",
    "        job['dataset'] = get_letor_string(job['dataset_params'])\n",
    "    elif \"sushi\" in job['dataset']:\n",
    "        job['dataset'] =  job['dataset']\n",
    "    else:\n",
    "        job['dataset'] = job['dataset_params']['dataset_type']\n",
    "    job['learner'] = job['learner'].upper()\n",
    "    job['dataset'] = job['dataset'].upper()\n",
    "    values = list(job.values())\n",
    "    keys = list(job.keys())\n",
    "    columns = keys[start:]\n",
    "    vals = values[start:]\n",
    "    data.append(vals)\n",
    "df_full = pd.DataFrame(data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>dataset</th>\n",
       "      <th>learner</th>\n",
       "      <th>categoricalaccuracy</th>\n",
       "      <th>categoricaltopk2</th>\n",
       "      <th>categoricaltopk3</th>\n",
       "      <th>categoricaltopk4</th>\n",
       "      <th>categoricaltopk5</th>\n",
       "      <th>categoricaltopk6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>413</td>\n",
       "      <td>SUSHI_DC</td>\n",
       "      <td>FETA_DC_SHALLOW_ZERO</td>\n",
       "      <td>0.2320</td>\n",
       "      <td>0.4130</td>\n",
       "      <td>0.4930</td>\n",
       "      <td>0.5715</td>\n",
       "      <td>0.6905</td>\n",
       "      <td>0.7570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>541</td>\n",
       "      <td>SUSHI_DC</td>\n",
       "      <td>FETA_DC_SHALLOW_ZERO</td>\n",
       "      <td>0.2445</td>\n",
       "      <td>0.3380</td>\n",
       "      <td>0.4690</td>\n",
       "      <td>0.5840</td>\n",
       "      <td>0.6760</td>\n",
       "      <td>0.7315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>538</td>\n",
       "      <td>SUSHI_DC</td>\n",
       "      <td>FETA_DC_SHALLOW_ZERO</td>\n",
       "      <td>0.1925</td>\n",
       "      <td>0.3695</td>\n",
       "      <td>0.5570</td>\n",
       "      <td>0.6225</td>\n",
       "      <td>0.7290</td>\n",
       "      <td>0.7745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>540</td>\n",
       "      <td>SUSHI_DC</td>\n",
       "      <td>FETA_DC_SHALLOW_ZERO</td>\n",
       "      <td>0.2570</td>\n",
       "      <td>0.3405</td>\n",
       "      <td>0.4320</td>\n",
       "      <td>0.5125</td>\n",
       "      <td>0.6210</td>\n",
       "      <td>0.7140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>539</td>\n",
       "      <td>SUSHI_DC</td>\n",
       "      <td>FETA_DC_SHALLOW_ZERO</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.3350</td>\n",
       "      <td>0.4715</td>\n",
       "      <td>0.5930</td>\n",
       "      <td>0.6880</td>\n",
       "      <td>0.7325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    job_id   dataset               learner  categoricalaccuracy  \\\n",
       "24     413  SUSHI_DC  FETA_DC_SHALLOW_ZERO               0.2320   \n",
       "6      541  SUSHI_DC  FETA_DC_SHALLOW_ZERO               0.2445   \n",
       "7      538  SUSHI_DC  FETA_DC_SHALLOW_ZERO               0.1925   \n",
       "14     540  SUSHI_DC  FETA_DC_SHALLOW_ZERO               0.2570   \n",
       "12     539  SUSHI_DC  FETA_DC_SHALLOW_ZERO               0.2565   \n",
       "\n",
       "    categoricaltopk2  categoricaltopk3  categoricaltopk4  categoricaltopk5  \\\n",
       "24            0.4130            0.4930            0.5715            0.6905   \n",
       "6             0.3380            0.4690            0.5840            0.6760   \n",
       "7             0.3695            0.5570            0.6225            0.7290   \n",
       "14            0.3405            0.4320            0.5125            0.6210   \n",
       "12            0.3350            0.4715            0.5930            0.6880   \n",
       "\n",
       "    categoricaltopk6  \n",
       "24            0.7570  \n",
       "6             0.7315  \n",
       "7             0.7745  \n",
       "14            0.7140  \n",
       "12            0.7325  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full = df_full.sort_values('dataset')\n",
    "#df_full['zeroonerankaccuracy'] = 1 - df_full['zeroonerankloss']\n",
    "df_full.loc[df_full['learner'] == 'FETA_DC_SHALLOW_ZERO']\n",
    "#df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_full[\"job_id\"]\n",
    "grouped = df_full.groupby(['dataset', 'learner'])\n",
    "data = []\n",
    "for name, group in grouped:\n",
    "    one_row = [name[0], str(name[1]).upper()]\n",
    "    #latex_row = [\"$ {}\".format(name[0]), \"$ {}\".format(str(name[1]).upper())]\n",
    "    std = group.std(axis=0).values\n",
    "    mean = group.mean(axis=0).values\n",
    "    if np.all(np.isnan(std)):\n",
    "        one_row.extend([\"{:.4f}\".format(m) for m in mean])\n",
    "        #latex_row.extend([\"${:.3f}$\".format(m) for m in mean]) \n",
    "    else:\n",
    "        std = [s*1e3 for s in std]\n",
    "        one_row.extend([\"{:.3f}({:.0f})\".format(m, s) for m, s in zip(mean, std)])\n",
    "        #latex_row.extend([\"$ {:.3f} \\pm {:.3f} \".format(m, s) for m, s in zip(mean, std)])\n",
    "    data.append(one_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>learner</th>\n",
       "      <th>categoricalaccuracy</th>\n",
       "      <th>categoricaltopk2</th>\n",
       "      <th>categoricaltopk3</th>\n",
       "      <th>categoricaltopk4</th>\n",
       "      <th>categoricaltopk5</th>\n",
       "      <th>categoricaltopk6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUSHI_DC</td>\n",
       "      <td>FATE_DC</td>\n",
       "      <td>0.292(21)</td>\n",
       "      <td>0.414(18)</td>\n",
       "      <td>0.559(78)</td>\n",
       "      <td>0.647(45)</td>\n",
       "      <td>0.730(35)</td>\n",
       "      <td>0.808(11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SUSHI_DC</td>\n",
       "      <td>FETA_DC_SHALLOW</td>\n",
       "      <td>0.292(7)</td>\n",
       "      <td>0.401(23)</td>\n",
       "      <td>0.507(26)</td>\n",
       "      <td>0.602(40)</td>\n",
       "      <td>0.687(32)</td>\n",
       "      <td>0.769(35)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SUSHI_DC</td>\n",
       "      <td>FETA_DC_SHALLOW_ZERO</td>\n",
       "      <td>0.237(27)</td>\n",
       "      <td>0.359(33)</td>\n",
       "      <td>0.484(46)</td>\n",
       "      <td>0.577(41)</td>\n",
       "      <td>0.681(39)</td>\n",
       "      <td>0.742(24)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SUSHI_DC</td>\n",
       "      <td>GENERALIZED_EXTREME_VALUE</td>\n",
       "      <td>0.218(62)</td>\n",
       "      <td>0.366(71)</td>\n",
       "      <td>0.502(18)</td>\n",
       "      <td>0.608(23)</td>\n",
       "      <td>0.685(22)</td>\n",
       "      <td>0.754(34)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SUSHI_DC</td>\n",
       "      <td>MIXED_LOGIT_MODEL</td>\n",
       "      <td>0.262(7)</td>\n",
       "      <td>0.387(8)</td>\n",
       "      <td>0.465(14)</td>\n",
       "      <td>0.566(13)</td>\n",
       "      <td>0.624(10)</td>\n",
       "      <td>0.724(14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SUSHI_DC</td>\n",
       "      <td>MULTINOMIAL_LOGIT_MODEL</td>\n",
       "      <td>0.271(6)</td>\n",
       "      <td>0.387(5)</td>\n",
       "      <td>0.502(2)</td>\n",
       "      <td>0.581(18)</td>\n",
       "      <td>0.676(11)</td>\n",
       "      <td>0.786(7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SUSHI_DC</td>\n",
       "      <td>NESTED_LOGIT_MODEL</td>\n",
       "      <td>0.263(12)</td>\n",
       "      <td>0.375(5)</td>\n",
       "      <td>0.492(14)</td>\n",
       "      <td>0.601(11)</td>\n",
       "      <td>0.671(13)</td>\n",
       "      <td>0.736(24)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SUSHI_DC</td>\n",
       "      <td>PAIRED_COMBINATORIAL_LOGIT</td>\n",
       "      <td>0.269(6)</td>\n",
       "      <td>0.387(6)</td>\n",
       "      <td>0.500(12)</td>\n",
       "      <td>0.595(18)</td>\n",
       "      <td>0.676(10)</td>\n",
       "      <td>0.785(6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SUSHI_DC</td>\n",
       "      <td>RANKNET_DC_SHALLOW</td>\n",
       "      <td>0.269(13)</td>\n",
       "      <td>0.436(20)</td>\n",
       "      <td>0.555(25)</td>\n",
       "      <td>0.661(14)</td>\n",
       "      <td>0.758(21)</td>\n",
       "      <td>0.831(20)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SUSHI_DC</td>\n",
       "      <td>RANKSVM_DC</td>\n",
       "      <td>0.258(4)</td>\n",
       "      <td>0.372(7)</td>\n",
       "      <td>0.480(22)</td>\n",
       "      <td>0.594(17)</td>\n",
       "      <td>0.679(13)</td>\n",
       "      <td>0.779(6)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset                     learner categoricalaccuracy categoricaltopk2  \\\n",
       "0  SUSHI_DC                     FATE_DC           0.292(21)        0.414(18)   \n",
       "1  SUSHI_DC             FETA_DC_SHALLOW            0.292(7)        0.401(23)   \n",
       "2  SUSHI_DC        FETA_DC_SHALLOW_ZERO           0.237(27)        0.359(33)   \n",
       "3  SUSHI_DC   GENERALIZED_EXTREME_VALUE           0.218(62)        0.366(71)   \n",
       "4  SUSHI_DC           MIXED_LOGIT_MODEL            0.262(7)         0.387(8)   \n",
       "5  SUSHI_DC     MULTINOMIAL_LOGIT_MODEL            0.271(6)         0.387(5)   \n",
       "6  SUSHI_DC          NESTED_LOGIT_MODEL           0.263(12)         0.375(5)   \n",
       "7  SUSHI_DC  PAIRED_COMBINATORIAL_LOGIT            0.269(6)         0.387(6)   \n",
       "8  SUSHI_DC          RANKNET_DC_SHALLOW           0.269(13)        0.436(20)   \n",
       "9  SUSHI_DC                  RANKSVM_DC            0.258(4)         0.372(7)   \n",
       "\n",
       "  categoricaltopk3 categoricaltopk4 categoricaltopk5 categoricaltopk6  \n",
       "0        0.559(78)        0.647(45)        0.730(35)        0.808(11)  \n",
       "1        0.507(26)        0.602(40)        0.687(32)        0.769(35)  \n",
       "2        0.484(46)        0.577(41)        0.681(39)        0.742(24)  \n",
       "3        0.502(18)        0.608(23)        0.685(22)        0.754(34)  \n",
       "4        0.465(14)        0.566(13)        0.624(10)        0.724(14)  \n",
       "5         0.502(2)        0.581(18)        0.676(11)         0.786(7)  \n",
       "6        0.492(14)        0.601(11)        0.671(13)        0.736(24)  \n",
       "7        0.500(12)        0.595(18)        0.676(10)         0.785(6)  \n",
       "8        0.555(25)        0.661(14)        0.758(21)        0.831(20)  \n",
       "9        0.480(22)        0.594(17)        0.679(13)         0.779(6)  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data, columns=columns[1:])\n",
    "df.sort_values(by='dataset')\n",
    "df_path = os.path.join(DIR_PATH, 'results' , DATASET+'.csv')\n",
    "df.to_csv(df_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_ranker(sub_df):\n",
    "    remove_ranker = None\n",
    "    if len(sub_df)==2:\n",
    "        sub_df = sub_df[:,1:3]\n",
    "        val1 = [float(x) for x in re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", sub_df[0][1])]\n",
    "        val2 = [float(x) for x in re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", sub_df[1][1])]\n",
    "        val1 = val1[0] if len(val1)==1 else val1[0] - val1[1]*1e-3\n",
    "        val2 = val2[0] if len(val2)==1 else val2[0] - val2[1]*1e-3\n",
    "        if val1 < val2 :\n",
    "            remove_ranker = sub_df[0][0]\n",
    "        else:\n",
    "            remove_ranker = sub_df[1][0]\n",
    "    \n",
    "    return remove_ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_best(df):\n",
    "    for col in list(df.columns)[1:]:\n",
    "        values_str = df[['learner',col]].as_matrix()\n",
    "        values = np.array([[val[0], float(val[1].split('(')[0])] for val in values_str])\n",
    "        maxi = np.argmax(values[:,1])\n",
    "        values_str[maxi] = [values_str[maxi][0], \"bfseries {}\".format(values_str[maxi][1])]\n",
    "        df['learner'] = values_str[:,0]\n",
    "        df[col] = values_str[:,1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name SUSHI_DC\n",
      "['learner', 'categoricalaccuracy', 'categoricaltopk2', 'categoricaltopk3', 'categoricaltopk4', 'categoricaltopk5', 'categoricaltopk6']\n",
      "\\begin{tabular}{lllllll}\n",
      "\\toprule\n",
      "learner & categoricalaccuracy & categoricaltopk2 & categoricaltopk3 & categoricaltopk4 & categoricaltopk5 & categoricaltopk6\\\\\n",
      "\\midrule\n",
      "\\pariwisesvm & 0.258(4) & 0.372(7) & 0.480(22) & 0.594(17) & 0.679(13) & 0.779(6)\\\\\n",
      "\\ranknetdc & 0.269(13) & \\bfseries 0.436(20) & 0.555(25) & \\bfseries 0.661(14) & \\bfseries 0.758(21) & \\bfseries 0.831(20)\\\\\n",
      "\\mnl & 0.271(6) & 0.387(5) & 0.502(2) & 0.581(18) & 0.676(11) & 0.786(7)\\\\\n",
      "\\nlm & 0.263(12) & 0.375(5) & 0.492(14) & 0.601(11) & 0.671(13) & 0.736(24)\\\\\n",
      "\\gev & 0.218(62) & 0.366(71) & 0.502(18) & 0.608(23) & 0.685(22) & 0.754(34)\\\\\n",
      "\\pcl & 0.269(6) & 0.387(6) & 0.500(12) & 0.595(18) & 0.676(10) & 0.785(6)\\\\\n",
      "\\mlm & 0.262(7) & 0.387(8) & 0.465(14) & 0.566(13) & 0.624(10) & 0.724(14)\\\\\n",
      "\\fate & \\bfseries 0.292(21) & 0.414(18) & \\bfseries 0.559(78) & 0.647(45) & 0.730(35) & 0.808(11)\\\\\n",
      "\\feta & 0.292(7) & 0.401(23) & 0.507(26) & 0.602(40) & 0.687(32) & 0.769(35)\\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from tabulate import tabulate\n",
    "import string\n",
    "grouped = df.groupby(['dataset'])\n",
    "for name, group in grouped:\n",
    "    \n",
    "    remove_rankers = []\n",
    "    sub_df = group[group['learner'].str.contains(\"FETA\")].as_matrix()\n",
    "    r1 = remove_ranker(sub_df)\n",
    "    sub_df = group[group['learner'].str.contains(\"FATE\")].as_matrix()\n",
    "    r2 = remove_ranker(sub_df)\n",
    "    sub_df = group[group['learner'].str.contains(\"RANKNET\")].as_matrix()\n",
    "    r3 = remove_ranker(sub_df)\n",
    "    remove_rankers.append(r1)\n",
    "    remove_rankers.append(r2)\n",
    "    remove_rankers.append(r3)\n",
    "    group = group[~group['learner'].isin(remove_rankers)]\n",
    "    group = group.replace({'FETA_DC_SHALLOW_ZERO': \"FETA_DC\"})\n",
    "    group = group.replace({'FATE_DC_SHALLOW': \"FATE_DC\"})\n",
    "    group = group.replace({'RANKNET_DC_SHALLOW': \"RANKNET_DC\"})\n",
    "    custom_dict = {\"RANKSVM_DC\":0, \"RANKNET_DC\":1, 'MULTINOMIAL_LOGIT_MODEL':2, 'NESTED_LOGIT_MODEL':3, 'GENERALIZED_EXTREME_VALUE':4, \n",
    "                   'PAIRED_COMBINATORIAL_LOGIT':5, \"MIXED_LOGIT_MODEL\":6, \"FATE_DC\":7, \"FETA_DC\":8, \"FETA_DC_ZERO\":9}\n",
    "    group['rank'] = group['learner'].map(custom_dict)\n",
    "    group.sort_values(by='rank', inplace=True)\n",
    "    del group[\"dataset\"]\n",
    "    del group['rank']\n",
    "    group = mark_best(group)\n",
    "    if len(group)==9:\n",
    "        group['learner'] = [\"pariwisesvm\", \"ranknetdc\", \"mnl\", \"nlm\", \"gev\", \"pcl\", \"mlm\", \"fate\", \"feta\"]\n",
    "    print(\"name {}\".format(name))\n",
    "    print(list(group.columns))\n",
    "    latex_code = group.to_latex(index = False)\n",
    "    latex_code = latex_code.replace(' ',\"\")\n",
    "    latex_code = latex_code.replace('&',\" & \")\n",
    "    latex_code = str(latex_code)\n",
    "    for learner in group['learner']:\n",
    "        latex_code = latex_code.replace(learner, \"\\\\{}\".format(learner))\n",
    "    latex_code = latex_code.replace(\"bfseries\", \"\\\\{} \".format(\"bfseries\"))\n",
    "    print(latex_code)\n",
    "#df.T.to_latex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = os.path.join(DIR_PATH, 'results' , \"discrete_choice.csv\")\n",
    "\n",
    "if not os.path.isfile(df_path):\n",
    "    dataFrame = df\n",
    "else:\n",
    "    dataFrame = pd.read_csv(df_path, index_col=0)\n",
    "    dataFrame = dataFrame.append(df, ignore_index=True)\n",
    "dataFrame\n",
    "dataFrame.to_csv(df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(['dataset'])\n",
    "for name, group in grouped:\n",
    "    df_path = os.path.join(DIR_PATH, 'results' , name.lower()+'.csv')\n",
    "    group.to_csv(df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.arange(48,87)\n",
    "\n",
    "X_train = np.arange(40).reshape(4,5,2)\n",
    "\n",
    "learner_params = {}\n",
    "learner_params['n_objects'], learner_params['n_object_features'] = X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from datetime import datetime\n",
    "self.schema = 'pymc3'\n",
    "avail_jobs = \"{}.avail_jobs\".format(self.schema)\n",
    "running_jobs = \"{}.running_jobs\".format(self.schema)\n",
    "fold_id = 1\n",
    "cluster_id=1234\n",
    "self.fetch_job_arguments(cluster_id=cluster_id)\n",
    "self.init_connection(cursor_factory=None)\n",
    "job_desc = dict(self.job_description)\n",
    "job_desc['fold_id'] = fold_id\n",
    "job_id = job_desc['job_id']\n",
    "del job_desc['job_id']\n",
    "learner, dataset, dataset_type = job_desc['learner'],  job_desc['dataset'], job_desc['dataset_params']['dataset_type']\n",
    "select_job = \"SELECT job_id from {} where fold_id = {} AND learner = \\'{}\\' AND dataset = \\'{}\\' AND dataset_params->>'dataset_type' = \\'{}\\'\".format(\n",
    "    avail_jobs, fold_id, learner, dataset, dataset_type)\n",
    "self.cursor_db.execute(select_job)\n",
    "\n",
    "if self.cursor_db.rowcount == 0:\n",
    "    keys = list(job_desc.keys())\n",
    "    columns = ', '.join(keys)\n",
    "    index = keys.index('fold_id')\n",
    "    keys[index] = str(fold_id)\n",
    "    values_str = ', '.join(keys)\n",
    "    insert_job = \"INSERT INTO {0} ({1}) SELECT {2} FROM {0} where {0}.job_id = {3} RETURNING job_id\".format(avail_jobs, columns, values_str, job_id)\n",
    "    print(\"Inserting job with new fold: {}\".format(insert_job))\n",
    "    self.cursor_db.execute(insert_job)    \n",
    "job_id = self.cursor_db.fetchone()[0]\n",
    "print(\"Job {} with fold id {} updated/inserted\".format(fold_id, job_id))\n",
    "start = datetime.now()\n",
    "update_job = \"\"\"UPDATE {} set job_allocated_time = %s WHERE job_id = %s\"\"\".format(avail_jobs)\n",
    "self.cursor_db.execute(update_job, (start, job_id))\n",
    "select_job = \"\"\"SELECT * FROM {0} WHERE {0}.job_id = {1} AND {0}.interrupted = {2} FOR UPDATE\"\"\".format(\n",
    "    running_jobs, job_id, True)\n",
    "self.cursor_db.execute(select_job)\n",
    "count_ = len(self.cursor_db.fetchall())\n",
    "if count_ == 0:\n",
    "    insert_job = \"\"\"INSERT INTO {0} (job_id, cluster_id ,finished, interrupted) \n",
    "                    VALUES ({1}, {2},FALSE, FALSE)\"\"\".format(running_jobs, job_id, cluster_id)\n",
    "    self.cursor_db.execute(insert_job)\n",
    "    if self.cursor_db.rowcount == 1:\n",
    "        print(\"The job {} is updated in runnung jobs\".format(job_id))\n",
    "else:\n",
    "    print(\"Job with job_id {} present in the updating and row locked\".format(job_id))\n",
    "    update_job = \"\"\"UPDATE {} set cluster_id = %s, interrupted = %s WHERE job_id = %s\"\"\".format(\n",
    "        running_jobs)\n",
    "    self.cursor_db.execute(update_job, (cluster_id, 'FALSE', job_id))\n",
    "    if self.cursor_db.rowcount == 1:\n",
    "        print(\"The job {} is updated in runnung jobs\".format(job_id))\n",
    "self.close_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unique_max_occurring'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"UNIQUE_MAX_OCCURRING\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
